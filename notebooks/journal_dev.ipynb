{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from app.analyzer import analyze_entry\n",
    "from app.storage import save_entry, load_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /opt/anaconda3/lib/python3.12/site-packages (1.37.1)\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.29.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Using cached streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
      "Installing collected packages: streamlit\n",
      "  Attempting uninstall: streamlit\n",
      "    Found existing installation: streamlit 1.37.1\n",
      "    Uninstalling streamlit-1.37.1:\n",
      "      Successfully uninstalled streamlit-1.37.1\n",
      "Successfully installed streamlit-1.45.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_text = \"\"\"\n",
    "I’ve been feeling overwhelmed by the pace of work lately. Even when I finish something, the to-do list just keeps growing. \n",
    "I know I should be proud of what I accomplish, but I can't help feeling like I'm falling short.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Entry saved at 2025-06-01T23:51:44.313319\n"
     ]
    }
   ],
   "source": [
    "result = analyze_entry(journal_text)\n",
    "save_entry(journal_text, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"\\nI’ve been feeling overwhelmed by the pace of work lately. Even when I finish something, the to-do list just keeps growing. \\nI know I should be proud of what I accomplish, but I can't help feeling like I'm falling short.\\n\",\n",
       " 'analysis': {'summary': 'Feeling overwhelmed by the pace of work which never seems to end.',\n",
       "  'emotions': ['Overwhelmed', 'Disappointment'],\n",
       "  'patterns': ['Catastrophizing', 'Self-Blame', 'Comparison'],\n",
       "  'themes': ['Work Stress'],\n",
       "  'ai_thoughts': \"Okay, so I need to analyze this journal entry and figure out the summary, emotions, cognitive distortions, and core themes. Let me read it carefully.\\n\\nThe person says they're overwhelmed by the pace of work. Even after finishing something, the list keeps growing. They know they should be proud but feel like they're falling short.\\n\\nFirst, summarizing: The main point is feeling overwhelmed with work tasks that never seem to end.\\n\\nEmotions: Overwhelmed and disappointment make sense because they can't finish tasks despite their pride in completing them.\\n\\nCognitive distortions: catastrophizing (worst-case scenario), self-Blame for not meeting expectations, maybe comparison (thinking others are more successful).\\n\\nCore themes: Work stress from the overwhelming workload and possible frustration with the work environment or colleagues.\",\n",
       "  'raw_output': '<think>\\nOkay, so I need to analyze this journal entry and figure out the summary, emotions, cognitive distortions, and core themes. Let me read it carefully.\\n\\nThe person says they\\'re overwhelmed by the pace of work. Even after finishing something, the list keeps growing. They know they should be proud but feel like they\\'re falling short.\\n\\nFirst, summarizing: The main point is feeling overwhelmed with work tasks that never seem to end.\\n\\nEmotions: Overwhelmed and disappointment make sense because they can\\'t finish tasks despite their pride in completing them.\\n\\nCognitive distortions: catastrophizing (worst-case scenario), self-Blame for not meeting expectations, maybe comparison (thinking others are more successful).\\n\\nCore themes: Work stress from the overwhelming workload and possible frustration with the work environment or colleagues.\\n</think>\\n\\n```json\\n{\\n  \"summary\": \"Feeling overwhelmed by the pace of work which never seems to end.\",\\n  \"emotions\": [\"Overwhelmed\", \"Disappointment\"],\\n  \"patterns\": [\"Catastrophizing\", \"Self-Blame\", \"Comparison\"],\\n  \"themes\": [\"Work Stress\"]\\n}\\n```'},\n",
       " 'timestamp': '2025-06-01T23:51:44.313319'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries = load_entries()\n",
    "entries[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.storage import load_entries\n",
    "\n",
    "entries = load_entries()\n",
    "\n",
    "# Find the entry by keyword (e.g., 'new Journal')\n",
    "for i, e in enumerate(reversed(entries)):\n",
    "    if \"new Journal\" in e[\"text\"]:\n",
    "        print(f\"Found at index: {-1 - i}\")\n",
    "        print(e[\"text\"])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = entries[-2]\n",
    "print(\"📝 Text:\\n\", entry[\"text\"])\n",
    "print(\"🧠 Summary:\\n\", entry[\"summary\"])\n",
    "print(\"🧾 Raw Output:\\n\", entry.get(\"raw_output\", \"No raw output found\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.analyzer import analyze_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyze_entry(entry['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_realistic_journals.py\n",
    "# Uses DeepSeek (via Ollama) to create 150 diverse journal entries\n",
    "\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "\n",
    "# Constants\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "MODEL_NAME = \"deepseek-r1\"\n",
    "OUTPUT_PATH = \"/Users/Additional Storage/ML Projects/Journal app/data/generated_deepseek_entries.jsonl\"\n",
    "\n",
    "# Emotion/theme seeds for diversity\n",
    "emotions_pool = [\n",
    "    \"anxiety\", \"joy\", \"loneliness\", \"hope\", \"confusion\", \"pride\", \"grief\", \"boredom\", \"motivation\",\n",
    "    \"excitement\", \"fear\", \"regret\", \"curiosity\", \"burnout\", \"peace\"\n",
    "]\n",
    "topics_pool = [\n",
    "    \"relationships\", \"work stress\", \"family dynamics\", \"career growth\", \"mental health\", \"self-discovery\",\n",
    "    \"routine fatigue\", \"health journey\", \"romantic feelings\", \"loss\", \"uncertainty about future\",\n",
    "    \"celebrating a win\", \"dealing with rejection\", \"spiritual questioning\"\n",
    "]\n",
    "\n",
    "# Prompt to generate realistic journal text\n",
    "ENTRY_GENERATION_PROMPT = \"\"\"\n",
    "You're a human writing a private journal.\n",
    "Your tone should be raw, informal, and personal.\n",
    "Write a journal entry about feeling {emotion} today, with thoughts related to {topic}.\n",
    "Include emotion, thought loops, and sensory detail if helpful.\n",
    "Keep it between 4–8 sentences.\n",
    "Do not include headings or lists.\n",
    "Just return the journal text.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "# Generate a single journal text from deepseek\n",
    "def generate_journal_text(emotion, topic):\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a human journaling assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": ENTRY_GENERATION_PROMPT.format(emotion=emotion, topic=topic)}\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        raw = response.json()[\"message\"][\"content\"].strip()\n",
    "        cleaned = re.sub(r\"<think>.*?</think>\", \"\", raw, flags=re.DOTALL).strip()\n",
    "        return cleaned\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to generate entry for {emotion} + {topic}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate realistic timestamp\n",
    "def random_timestamp(start_year=2024, end_year=2025):\n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    total_days = (end_date - start_date).days\n",
    "    random_days = random.randint(0, total_days)\n",
    "    random_seconds = random.randint(0, 86400)\n",
    "    return (start_date + timedelta(days=random_days, seconds=random_seconds)).isoformat()\n",
    "\n",
    "# Generate entries and save\n",
    "entries = []\n",
    "combos = list(set((e, t) for e in emotions_pool for t in topics_pool))\n",
    "random.shuffle(combos)\n",
    "\n",
    "total = 2\n",
    "print(f\"🧠 Generating {total} entries via DeepSeek…\")\n",
    "\n",
    "for i in range(total):\n",
    "    emotion, topic = combos[i % len(combos)]\n",
    "    text = generate_journal_text(emotion, topic)\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    entry = {\n",
    "        \"timestamp\": random_timestamp(),\n",
    "        \"text\": text,\n",
    "        \"summary\": \"\",  # To be filled by your analyzer\n",
    "        \"emotions\": [],\n",
    "        \"patterns\": [],\n",
    "        \"themes\": [],\n",
    "        \"ai_thoughts\": \"\",\n",
    "        \"raw_output\": \"\"\n",
    "    }\n",
    "    entries.append(entry)\n",
    "    print(f\"✅ {i+1}/{total}: {emotion}, {topic}\")\n",
    "    time.sleep(1.5)  # Optional pacing if DeepSeek needs time\n",
    "\n",
    "# Save to .jsonl\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    for entry in entries:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"\\n✅ Done. Saved {len(entries)} entries to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "\n",
    "with open(OUTPUT_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        entries.append(json.loads(line.strip()))\n",
    "\n",
    "# Now you can access individual ones like:\n",
    "print(entries[0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "\n",
    "with open(OUTPUT_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        entries.append(json.loads(line.strip()))\n",
    "\n",
    "# Now you can access individual ones like:\n",
    "print(entries[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "\n",
    "with open(OUTPUT_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        entries.append(json.loads(line.strip()))\n",
    "\n",
    "# Now you can access individual ones like:\n",
    "print(entries[5]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_realistic_journals.py\n",
    "import json, random, re, requests\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "MODEL_NAME = \"deepseek-r1\"\n",
    "OUTPUT_PATH = \"/Users/Additional Storage/ML Projects/Journal app/data/test_generated_entries.jsonl\"\n",
    "TOTAL_ENTRIES = 100\n",
    "TEMPERATURE = 0.8\n",
    "\n",
    "emotions = [\"anxiety\", \"joy\", \"confusion\", \"hope\", \"frustration\", \"nostalgia\", \"burnout\", \"contentment\", \"loneliness\"]\n",
    "topics = [\"work stress\", \"relationships\", \"self-doubt\", \"mental health\", \"growth\", \"grief\", \"career uncertainty\", \"romantic confusion\", \"burnout recovery\", \"loneliness\", \"change and transition\", \"creative block\", \"financial pressure\", \"family expectations\", \"isolation\", \"identity\", \"body image\", \"success anxiety\", \"fear of failure\", \"disconnection from others\", \"social anxiety\"]\n",
    "\n",
    "GEN_PROMPT = \"\"\"\n",
    "You are a private journal writer.\n",
    "Write a thoughtful, emotionally realistic journal entry from someone who is experiencing the emotion: {emotion}, in the context of: {topic}.\n",
    "The tone should feel personal and introspective — not scripted / cmputer written, it should sound like a normal decently educated indian english writer.\n",
    "Write 4–8 flowing sentences, including sensory details and authentic reflection.\n",
    "Ensure good grammar and spelling, and avoid artificial phrasing or markdown.\n",
    "Return only the journal entry text with no commentary or extra formatting.\n",
    "\"\"\"\n",
    "\n",
    "def generate_entry(emotion, topic):\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a human journaling assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": GEN_PROMPT.format(emotion=emotion, topic=topic)}\n",
    "        ],\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        res = requests.post(OLLAMA_URL, json=payload)\n",
    "        raw = res.json()[\"message\"][\"content\"].strip()\n",
    "        return re.sub(r\"<think>.*?</think>\", \"\", raw, flags=re.DOTALL).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for {emotion}/{topic}: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_entry(text):\n",
    "    ANALYSIS_PROMPT = \"\"\"\n",
    "You are a compassionate AI assistant trained to analyze journal entries.\n",
    "Return ONLY a valid raw JSON object with the following keys:\n",
    "  - summary: a one-sentence paraphrase of the overall entry\n",
    "  - emotions: list of 1–3 dominant emotional states\n",
    "  - patterns: list of up to 3 cognitive distortions (e.g., catastrophizing)\n",
    "  - themes: list of up to 3 core life topics the entry relates to\n",
    "\n",
    "Do NOT include explanations or markdown.\n",
    "\"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": ANALYSIS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        res = requests.post(OLLAMA_URL, json=payload)\n",
    "        content = res.json()[\"message\"][\"content\"].strip()\n",
    "        match = re.search(r\"{.*}\", content, re.DOTALL)\n",
    "        return json.loads(match.group(0)) if match else None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Analysis error: {e}\")\n",
    "        return None\n",
    "\n",
    "def random_timestamp():\n",
    "    start = datetime(2024, 1, 1)\n",
    "    end = datetime(2024, 12, 31)\n",
    "    delta = end - start\n",
    "    offset = timedelta(days=random.randint(0, delta.days), hours=random.randint(18, 22), minutes=random.randint(0, 59))\n",
    "    return (start + offset).isoformat()\n",
    "\n",
    "def emotion_for_month(month):\n",
    "    bias_map = {\n",
    "        1: [\"anxiety\", \"burnout\"], 2: [\"anxiety\"], 3: [\"confusion\"], 4: [\"hope\"], 5: [\"burnout\"],\n",
    "        6: [\"joy\", \"contentment\"], 7: [\"joy\"], 8: [\"nostalgia\"], 9: [\"confusion\", \"anxiety\"],\n",
    "        10: [\"nostalgia\", \"loneliness\"], 11: [\"loneliness\"], 12: [\"loneliness\", \"contentment\"]\n",
    "    }\n",
    "    biased = bias_map.get(month, [])\n",
    "    return random.choice(biased) if biased and random.random() < 0.6 else random.choice(emotions)\n",
    "\n",
    "# Load existing\n",
    "existing = []\n",
    "if Path(OUTPUT_PATH).exists():\n",
    "    with open(OUTPUT_PATH) as f:\n",
    "        existing = [json.loads(line) for line in f if line.strip()]\n",
    "existing_ts = set(e[\"timestamp\"] for e in existing)\n",
    "\n",
    "# Generate\n",
    "new_entries = []\n",
    "for _ in range(TOTAL_ENTRIES):\n",
    "    ts = random_timestamp()\n",
    "    if ts in existing_ts:\n",
    "        continue\n",
    "    emo = emotion_for_month(datetime.fromisoformat(ts).month)\n",
    "    top = random.choice(topics)\n",
    "    text = generate_entry(emo, top)\n",
    "    if not text:\n",
    "        continue\n",
    "    analysis = analyze_entry(text)\n",
    "    if not analysis:\n",
    "        continue\n",
    "    new_entries.append({\n",
    "        \"timestamp\": ts,\n",
    "        \"text\": text,\n",
    "        \"analysis\": analysis\n",
    "    })\n",
    "    print(f\"✅ {ts} — {emo} | {top}\")\n",
    "\n",
    "# Save all\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    for e in existing + new_entries:\n",
    "        f.write(json.dumps(e) + \"\\n\")\n",
    "\n",
    "print(f\"\\n📄 Saved {len(new_entries)} new entries to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created /Users/Additional Storage/ML Projects/Journal app/data/rag_journal_entries.jsonl with 1 entries.\n",
      "✅ Created /Users/Additional Storage/ML Projects/Journal app/data/rag_test_generated_entries.jsonl with 119 entries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "DATA_DIR = \"/Users/Additional Storage/ML Projects/Journal app/data\"\n",
    "\n",
    "for filename in os.listdir(DATA_DIR):\n",
    "    if filename.endswith(\".jsonl\") and not filename.startswith(\"rag_\"):\n",
    "        input_path = os.path.join(DATA_DIR, filename)\n",
    "        output_path = os.path.join(DATA_DIR, f\"rag_{filename}\")\n",
    "\n",
    "        rag_entries = []\n",
    "        with open(input_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    entry = json.loads(line.strip())\n",
    "                    rag_doc = {\n",
    "                        \"content\": entry.get(\"text\", \"\"),\n",
    "                        \"metadata\": {\n",
    "                            \"timestamp\": entry.get(\"timestamp\"),\n",
    "                            \"themes\": entry.get(\"analysis\", {}).get(\"themes\", []),\n",
    "                            \"emotions\": entry.get(\"analysis\", {}).get(\"emotions\", [])\n",
    "                        }\n",
    "                    }\n",
    "                    rag_entries.append(rag_doc)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"❌ Skipped malformed line in {filename}\")\n",
    "\n",
    "        with open(output_path, \"w\") as f:\n",
    "            for doc in rag_entries:\n",
    "                f.write(json.dumps(doc) + \"\\n\")\n",
    "\n",
    "        print(f\"✅ Created {output_path} with {len(rag_entries)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['journal_entries.jsonl', 'test_generated_entries.jsonl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"/Users/Additional Storage/ML Projects/Journal app/data\"\n",
    "jsonl_files = [f for f in os.listdir(data_dir) if f.endswith(\".jsonl\") and not f.startswith(\"rag_\")]\n",
    "print(jsonl_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Checking: journal_entries.jsonl\n",
      "\n",
      "📂 Checking: test_generated_entries.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for file in jsonl_files:\n",
    "    print(f\"\\n📂 Checking: {file}\")\n",
    "    path = os.path.join(data_dir, file)\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                obj = json.loads(line.strip())\n",
    "                if \"timestamp\" not in obj:\n",
    "                    print(f\"❌ Missing timestamp at line {i+1}: {obj}\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Malformed JSON at line {i+1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1 clean entries\n",
      "❌ 0 bad entries\n"
     ]
    }
   ],
   "source": [
    "clean_entries = []\n",
    "bad_entries = []\n",
    "\n",
    "with open(os.path.join(data_dir, \"journal_entries.jsonl\")) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            obj = json.loads(line.strip())\n",
    "            if \"timestamp\" in obj:\n",
    "                clean_entries.append(obj)\n",
    "            else:\n",
    "                bad_entries.append((i + 1, obj))\n",
    "        except json.JSONDecodeError as e:\n",
    "            bad_entries.append((i + 1, f\"Malformed JSON: {e}\"))\n",
    "\n",
    "print(f\"✅ {len(clean_entries)} clean entries\")\n",
    "print(f\"❌ {len(bad_entries)} bad entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Checking: journal_entries.jsonl\n",
      "\n",
      "📁 Checking: test_generated_entries.jsonl\n"
     ]
    }
   ],
   "source": [
    "for file in jsonl_files:\n",
    "    print(f\"\\n📁 Checking: {file}\")\n",
    "    path = os.path.join(data_dir, file)\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                obj = json.loads(line.strip())\n",
    "                if \"timestamp\" not in obj:\n",
    "                    print(f\"❌ {file} line {i+1} is missing timestamp:\\n{obj}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error in {file} line {i+1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = [json.loads(line.strip()) for line in open(path)]\n",
    "entries.sort(key=lambda x: x[\"timestamp\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking: test_generated_entries.jsonl\n",
      "Line 01 → ✅ timestamp\n",
      "Line 02 → ✅ timestamp\n",
      "Line 03 → ✅ timestamp\n",
      "Line 04 → ✅ timestamp\n",
      "Line 05 → ✅ timestamp\n",
      "Line 06 → ✅ timestamp\n",
      "Line 07 → ✅ timestamp\n",
      "Line 08 → ✅ timestamp\n",
      "Line 09 → ✅ timestamp\n",
      "Line 10 → ✅ timestamp\n",
      "Line 11 → ✅ timestamp\n",
      "Line 12 → ✅ timestamp\n",
      "Line 13 → ✅ timestamp\n",
      "Line 14 → ✅ timestamp\n",
      "Line 15 → ✅ timestamp\n",
      "Line 16 → ✅ timestamp\n",
      "Line 17 → ✅ timestamp\n",
      "Line 18 → ✅ timestamp\n",
      "Line 19 → ✅ timestamp\n",
      "Line 20 → ✅ timestamp\n",
      "Line 21 → ✅ timestamp\n",
      "Line 22 → ✅ timestamp\n",
      "Line 23 → ✅ timestamp\n",
      "Line 24 → ✅ timestamp\n",
      "Line 25 → ✅ timestamp\n",
      "Line 26 → ✅ timestamp\n",
      "Line 27 → ✅ timestamp\n",
      "Line 28 → ✅ timestamp\n",
      "Line 29 → ✅ timestamp\n",
      "Line 30 → ✅ timestamp\n",
      "Line 31 → ✅ timestamp\n",
      "Line 32 → ✅ timestamp\n",
      "Line 33 → ✅ timestamp\n",
      "Line 34 → ✅ timestamp\n",
      "Line 35 → ✅ timestamp\n",
      "Line 36 → ✅ timestamp\n",
      "Line 37 → ✅ timestamp\n",
      "Line 38 → ✅ timestamp\n",
      "Line 39 → ✅ timestamp\n",
      "Line 40 → ✅ timestamp\n",
      "Line 41 → ✅ timestamp\n",
      "Line 42 → ✅ timestamp\n",
      "Line 43 → ✅ timestamp\n",
      "Line 44 → ✅ timestamp\n",
      "Line 45 → ✅ timestamp\n",
      "Line 46 → ✅ timestamp\n",
      "Line 47 → ✅ timestamp\n",
      "Line 48 → ✅ timestamp\n",
      "Line 49 → ✅ timestamp\n",
      "Line 50 → ✅ timestamp\n",
      "Line 51 → ✅ timestamp\n",
      "Line 52 → ✅ timestamp\n",
      "Line 53 → ✅ timestamp\n",
      "Line 54 → ✅ timestamp\n",
      "Line 55 → ✅ timestamp\n",
      "Line 56 → ✅ timestamp\n",
      "Line 57 → ✅ timestamp\n",
      "Line 58 → ✅ timestamp\n",
      "Line 59 → ✅ timestamp\n",
      "Line 60 → ✅ timestamp\n",
      "Line 61 → ✅ timestamp\n",
      "Line 62 → ✅ timestamp\n",
      "Line 63 → ✅ timestamp\n",
      "Line 64 → ✅ timestamp\n",
      "Line 65 → ✅ timestamp\n",
      "Line 66 → ✅ timestamp\n",
      "Line 67 → ✅ timestamp\n",
      "Line 68 → ✅ timestamp\n",
      "Line 69 → ✅ timestamp\n",
      "Line 70 → ✅ timestamp\n",
      "Line 71 → ✅ timestamp\n",
      "Line 72 → ✅ timestamp\n",
      "Line 73 → ✅ timestamp\n",
      "Line 74 → ✅ timestamp\n",
      "Line 75 → ✅ timestamp\n",
      "Line 76 → ✅ timestamp\n",
      "Line 77 → ✅ timestamp\n",
      "Line 78 → ✅ timestamp\n",
      "Line 79 → ✅ timestamp\n",
      "Line 80 → ✅ timestamp\n",
      "Line 81 → ✅ timestamp\n",
      "Line 82 → ✅ timestamp\n",
      "Line 83 → ✅ timestamp\n",
      "Line 84 → ✅ timestamp\n",
      "Line 85 → ✅ timestamp\n",
      "Line 86 → ✅ timestamp\n",
      "Line 87 → ✅ timestamp\n",
      "Line 88 → ✅ timestamp\n",
      "Line 89 → ✅ timestamp\n",
      "Line 90 → ✅ timestamp\n",
      "Line 91 → ✅ timestamp\n",
      "Line 92 → ✅ timestamp\n",
      "Line 93 → ✅ timestamp\n",
      "Line 94 → ✅ timestamp\n",
      "Line 95 → ✅ timestamp\n",
      "Line 96 → ✅ timestamp\n",
      "Line 97 → ✅ timestamp\n",
      "Line 98 → ✅ timestamp\n",
      "Line 99 → ✅ timestamp\n",
      "Line 100 → ✅ timestamp\n",
      "Line 101 → ✅ timestamp\n",
      "Line 102 → ✅ timestamp\n",
      "Line 103 → ✅ timestamp\n",
      "Line 104 → ✅ timestamp\n",
      "Line 105 → ✅ timestamp\n",
      "Line 106 → ✅ timestamp\n",
      "Line 107 → ✅ timestamp\n",
      "Line 108 → ✅ timestamp\n",
      "Line 109 → ✅ timestamp\n",
      "Line 110 → ✅ timestamp\n",
      "Line 111 → ✅ timestamp\n",
      "Line 112 → ✅ timestamp\n",
      "Line 113 → ✅ timestamp\n",
      "Line 114 → ✅ timestamp\n",
      "Line 115 → ✅ timestamp\n",
      "Line 116 → ✅ timestamp\n",
      "Line 117 → ✅ timestamp\n",
      "Line 118 → ✅ timestamp\n",
      "Line 119 → ✅ timestamp\n",
      "\n",
      "Summary: 0 bad entries\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Step 1 – Point to the file that is causing the issue\n",
    "path = \"/Users/Additional Storage/ML Projects/Journal app/data/test_generated_entries.jsonl\"  # or whichever is selected in the app\n",
    "\n",
    "# Step 2 – Print each line and highlight whether 'timestamp' exists\n",
    "bad_entries = []\n",
    "\n",
    "print(f\"🔍 Checking: {os.path.basename(path)}\")\n",
    "with open(path, \"r\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            has_timestamp = \"timestamp\" in entry\n",
    "            print(f\"Line {i:02d} → {'✅' if has_timestamp else '❌'} timestamp\", end=\"\")\n",
    "            if not has_timestamp:\n",
    "                print(f\" | keys: {list(entry.keys())}\")\n",
    "                bad_entries.append((i, entry))\n",
    "            else:\n",
    "                print()\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ Line {i:02d} → JSON error: {e}\")\n",
    "            bad_entries.append((i, None))\n",
    "\n",
    "# Step 3 – Summary\n",
    "print(f\"\\nSummary: {len(bad_entries)} bad entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries: 2\n",
      "Last entry preview: [{'content': \"I am checking if the new RAG works, I hope it does and le'ts see - I am nervous about it 123456789 practicing counting numbers\", 'metadata': {'timestamp': '2025-06-04T22:36:40.885776', 'themes': ['anxiety about technology', 'practicing a routine task', 'new tool anxiety'], 'emotions': ['anxiety', 'nervousness']}}, {'content': \"I am checking if the new RAG works, I hope it does and le'ts see - I am nervous about it 123456789 practicing counting numbers let's try something new may be that will workkk umm lets see RAG\\nNLP\", 'metadata': {'timestamp': '2025-06-04T22:38:19.492991', 'themes': ['testing RAG model', 'trying something new', 'experimentation with AI'], 'emotions': ['nervousness', 'uncertainty', 'excitement']}}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "rag_path = '/Users/Additional Storage/ML Projects/Journal app/data/rag/rag_journal_entries.jsonl'\n",
    "\n",
    "with open(rag_path, \"r\") as f:\n",
    "    lines = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total entries: {len(lines)}\")\n",
    "print(\"Last entry preview:\", lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Match 1:\n",
      "Text: I am checking if the new RAG works, I hope it does and le'ts see - I am nervous about it 123456789 practicing counting numbers let's try something new may be that will workkk umm lets see RAG\n",
      "NLP\n",
      "Metadata: {'timestamp': '2025-06-04T22:38:19.492991', 'themes': ['testing RAG model', 'trying something new', 'experimentation with AI'], 'emotions': ['nervousness', 'uncertainty', 'excitement']}\n",
      "\n",
      "Match 2:\n",
      "Text: I am checking if the new RAG works, I hope it does and le'ts see - I am nervous about it 123456789 practicing counting numbers\n",
      "Metadata: {'timestamp': '2025-06-04T22:36:40.885776', 'themes': ['anxiety about technology', 'practicing a routine task', 'new tool anxiety'], 'emotions': ['anxiety', 'nervousness']}\n"
     ]
    }
   ],
   "source": [
    "# query_rag_journal.py\n",
    "\n",
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Path to vector store folder\n",
    "RAG_FOLDER = \"/Users/Additional Storage/ML Projects/Journal app/data/rag/rag_journal_entries\"\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load FAISS index\n",
    "db = FAISS.load_local(\n",
    "    folder_path=RAG_FOLDER,\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "def query_journal(question: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant journal entries for a given semantic query.\n",
    "    Returns a list of (text, metadata) tuples.\n",
    "    \"\"\"\n",
    "    results = db.similarity_search(question, k=k)\n",
    "    return [(doc.page_content, doc.metadata) for doc in results]\n",
    "\n",
    "# Example usage\n",
    "q = \"What are my recurring themes about self-worth?\"\n",
    "matches = query_journal(q)\n",
    "for i, (text, meta) in enumerate(matches, 1):\n",
    "    print(f\"\\nMatch {i}:\")\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Metadata:\", meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
